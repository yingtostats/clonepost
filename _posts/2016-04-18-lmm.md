---
layout: post
title:  "Linear Mixed Model"
date:   2016-04-20 21:49:00
tag:
- Statistics
- MixedModel
projects: true
blog: false
author: Jannis
description: Linear Mixed Model
fontsize: 23pt

---

<font size="3.5">
<blockquote>
All models are wrong, but some are useful.
-- George E.P. Box </blockquote>
</font>
<br>

 {% include mathjax_support.html %}

### Estimate $\psi$ via Generalized Estimating Equations (GEE)

By Theorem 4.3 of Jiang (2007), Suppose that V is known, and that $E(\dot{\mu}^{\prime} V^{-1}\dot{\mu})$ is non-singular. Then, the optimal estimating function within H is given by <span>$G^{*} = \dot{\mu}^{\prime} V^{-1}(y-\mu)</span>, that is, with $A = A^{*} = \dot{\mu}^{\prime} V^{-1}$.

Here the optimality is in a similar sense to the univariate case. Define the partial order of nonnegative definite matrices as A $\geq$ B if A - B is nonnegative definite. Then, the optimality in Theorem 4.3 is in the sense that the estimating function $G^{*}$ maximizes, in the partial order of nonnegative definite matrices, the generalized information criterion
$$I(G) = \{E(\dot{G})\}^{\prime} \{E(GG^{\prime})\}^{-1} \{E(\dot{G})\}$$
where $\dot{G} = \partial G/\partial \theta ^{\prime}$.  

The longitudinal GLMM, the optimal estimating function according to Theorem 4.3 can be expressed as $$ G^{*} = \sum_{i=1}^{m} \dot{\mu}_{i}^{\prime} V_{i}^{-1} (y_{i} - \mu_{i})= 0 $$
where

* $y_{i} = (y_{ij})_{1\leq j \leq 4}$

* $\mu_{i} = E(y_{i}) = (\mu_{ij})_{1\leq j \leq 4}$


* $$\dot{\mu}_{i} = \{\frac{\partial \mu_{ij}}{\partial \psi_{k}}\}_{1 \leq j \leq 4, 1 \leq k \leq 11} = \begin{pmatrix}
        \frac{\partial \mu_{i1}}{\partial \beta_{0}} & \frac{\partial \mu_{i1}}{\partial \beta_{1}} & ... & \frac{\partial \mu_{i1}}{\partial \beta_{6}} & \frac{\partial \mu_{i1}}{\partial \sigma_{1}} & ... & \frac{\partial \mu_{i1}}{\partial \tau} \\
        . & . & & . & . & & . \\
       . & . & ... & . & . & ... & . \\  
        . & . & & . & . & & . \\
  \frac{\partial \mu_{i1}}{\partial \beta_{0}} & \frac{\partial \mu_{i1}}{\partial \beta_{1}} & ... & \frac{\partial \mu_{i1}}{\partial \beta_{6}} & \frac{\partial \mu_{i1}}{\partial \sigma_{1}} & ... & \frac{\partial \mu_{i1}}{\partial \tau}
  \end{pmatrix}
      $$


* $V_{i} = Var(y_{i})$.

However, $V_{i}$, $1 \leq i \leq 59$, are unknown in practice. Liang and Zeger(1986) proposed replacing $V_{i}$'s with "working" covariance matrices in order to solve equation. They showed that under regualarity conditions,the resulting GEE estimator is consisitent even though the working covariance matrices misspecify the true $V_{i}$'s.


For simplicity, replace $V_{i}$ with $I_{4}$ and solve (1). That is, solve $$G_{I}^{*}=\sum_{i=1}^{59} \dot{\mu}_{i}^{\prime} (y_{i} - \mu_{i}) = 0$$

We can derive $\mu_{i}$ and $\dot{\mu}_{i}$ analytically:
$$E(\mu_{ij} \mid \alpha_{i}) = exp(x_{ij}^{\prime} + \alpha_{i1} + \alpha_{i2}v_{j} + \epsilon) = exp(x_{ij}\beta + \xi_{ij})$$ where $\xi_{ij} ~ N(0, \sigma_{1}^{2} + v^{2}_{j}  \sigma^{2}_{2} + 2v_{j} \rho \sigma_{1} \sigma_{2} + \tau^{2})$.

Therefore, using the law of total expectation and moment generating function of a normal random variable,
$$\mu_{ij} = E\{ exp(x_{ij}\beta + \xi_{ij})\} = exp(x_{ij}\beta)E(e^{\xi_{ij}}) = exp\{x_{ij}\beta + \frac{1}{2}(\sigma_{1}^{2} + v^{2}_{j}\sigma^{2}_{2} + 2v_{j}\rho\sigma_{1}\sigma_{2} + \tau^{2})\}$$

Let$x_{ijk}$ be the $k^{th}$ component of $x_{ij}$, $1 \leq k \leq 7$. Then,
$$\begin{aligned}
\frac{\partial \mu_{ij}}{\partial \beta_{k-1}} &= \mu_{ij}x_{ijk}, 1 \leq k \leq 7 \\
\frac{\partial \mu_{ij}}{\partial \sigma_{1}} &= \mu_{ij}(\sigma_{1} + v_{j}\rho \sigma_{2}) \\
\frac{\partial \mu_{ij}}{\partial \sigma_{2}} &= \mu_{ij}(v_{j}^{2}\sigma_{2} + v_{j}\rho \sigma_{1}) \\
\frac{\partial \mu_{ij}}{\partial \rho} &= \mu_{ij}v_{j}\sigma_{1}\sigma_{2} \\
\frac{\partial \mu_{ij}}{\partial \tau } &= \mu_{ij}\tau
\end{aligned}$$

Now, solve for $G^{*}_{I} = 0$ with the constraints $\sigma_{1} > 0$, $\sigma_{2} > 0$,
$\rho \in [-1,1]$, $\beta \in \textbf{R}^{7}$. This is an 11-dimensional nonlinear equation.

### Why GEE better for this model?

* The computational difficulty of ML estimation has made approaches based on general estimating equations attractive. GEEs are a computationally less demanding method than ML equation.

* Also, the efficiency of the likelihood-based methods may be undermined in the case of model misspecification, which often occurs in the analysis of longitudinal data. In longitudianl studies there often exists serial correlation among the repeated measures from the same subject. Such a serial correlation may not be taken into account by a GLMM. Note that, under the GLMM assumption, the $y_{ij} \mid (\alpha_{i},\epsilon_{ij})$ are conditionally independent given the random effects, which means that no (additional) serial correlation exists once the value of the random effects are specified. However, serial correlation may exist among the repeated reponses even given the random effects. In other words, the true correaltions among the data may not have been adequately addressed by the GLMMs. GEE is applicable the cases beyond the scope of GLMM.

* We don't have to specify the covariance matrix V correctly to obtain a reasonable estimates of $\psi$. A "working" covariance matrices can also result in a consistent GEE estimator.
